{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2800d6a-af19-43a2-8d50-499d97e92e9d"}}},{"cell_type":"markdown","source":["# Summary and Next Steps\nCongratulations!\nYou have completed Lakehouse with Delta Lake Deep Dive hands-on component.\n\nAt this point, we invite you to think about the work we have done and how it relates to the full IoT data ingestion pipeline we have been designing.\n\nIn this course, we used Spark SQL and Delta Lake to do the following to create a Single Source of Truth in our EDSS, the `health_tracker_processed` Delta table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cc02908-2f3d-4663-b74e-61deafb54d47"}}},{"cell_type":"markdown","source":["We did this through the following steps:\n- We converted an existing Parquet-based data lake table to a Delta table, health_tracker_processed.\n- We performed a batch upload of new data to this table.\n- We used Apache Spark to identify broken and missing records in this table.\n- We used Delta Lake’s ability to do an upsert, where we updated broken records and inserted missing records.\n- We evolved the schema of the Delta table.\n- We used Delta Lake’s Time Travel feature to scrub the personal data of a user intelligently.\n\nAdditionally, we used Delta Lake to create an aggregate table, `health_tracker_user_analytics`, downstream from the `health_tracker_processed` table.\n\nNow, return to the course to learn more about best practices for optimizing performance in your Lakehouse."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d10e21e-ed35-4862-9433-16151fc70956"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5beb276-f3c5-41c0-a9e4-8c3e36ac33cd"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"09_summary_and_next_steps","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2261997875603954}},"nbformat":4,"nbformat_minor":0}
